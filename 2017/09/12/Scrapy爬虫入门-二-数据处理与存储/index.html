<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Scrapy爬虫入门[二] | 数据处理与存储 · 末小山 | 胡言乱语 万念俱寂</title><meta name="description" content="Scrapy爬虫入门[二] | 数据处理与存储 - Mihawkmah"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://Mihawkmah.github.io/atom.xml" title="末小山 | 胡言乱语 万念俱寂"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">我的</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">胶囊</a></li><li class="nav-list-item"><a href="https://weibo.com/mihawkmah" target="_blank" class="nav-list-link">微博</a></li><li class="nav-list-item"><a href="http://mihawkmah.lofter.com/" target="_blank" class="nav-list-link">摄影</a></li><li class="nav-list-item"><a href="https://github.com/mihawkmah" target="_blank" class="nav-list-link">足迹</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Scrapy爬虫入门[二] | 数据处理与存储</h1><div class="post-info">2017年9月12日</div><div class="post-content"><h3 id="使用项目item"><a href="#使用项目item" class="headerlink" title="使用项目item"></a>使用项目item</h3><p>Scrapy提供了item类，是装载抓取数据的容器。声明类似于Flask框架中的Models。在tutorial目录下的iterms.py文件编辑。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line">class Product(scrapy.Item):</div><div class="line">    name = scrapy.Field()</div><div class="line">    price = scrapy.Field()</div><div class="line">    stock = scrapy.Field()</div><div class="line">    last_updated = scrapy.Field(serializer=str)</div></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">product [ &apos;last_updated&apos; ]  =  &apos;today&apos;</div></pre></td></tr></table></figure>
<h4 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 查询product的数据主键</div><div class="line">product.keys()</div><div class="line"># 查询product中的所有数据</div><div class="line">product.items()</div></pre></td></tr></table></figure>
<h4 id="复制项目"><a href="#复制项目" class="headerlink" title="复制项目"></a>复制项目</h4><p><code>product2 = product.copy()</code></p>
<h4 id="从项目创建字典"><a href="#从项目创建字典" class="headerlink" title="从项目创建字典"></a>从项目创建字典</h4><p><code>dict(product)</code></p>
<h3 id="使用项目管道item-Pipeline"><a href="#使用项目管道item-Pipeline" class="headerlink" title="使用项目管道item Pipeline"></a>使用项目管道item Pipeline</h3><p>项目被蜘蛛住区后，它被发送到项目管道</p>
<h4 id="典型用途"><a href="#典型用途" class="headerlink" title="典型用途"></a>典型用途</h4><ul>
<li>清理HTML数据验证</li>
<li>验证爬取的数据（检查项目是否包含特定字段）</li>
<li>查重（并删除）</li>
<li>将刮取的项目存储在数据库中</li>
</ul>
<h4 id="pipelines-py中编写item-pipeline"><a href="#pipelines-py中编写item-pipeline" class="headerlink" title="pipelines.py中编写item pipeline"></a>pipelines.py中编写item pipeline</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">class DoubanPipeline(object):</div><div class="line">    def process_item(self, item, spider):</div><div class="line">        return item</div></pre></td></tr></table></figure>
<p>在settings.py中进行注册</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">   &apos;myproject.pipelines.DoubanPipeline&apos;: 300,</div><div class="line">   &apos;myproject.pipelines.DouDouPipline&apos;:500,</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内。</p>
<h3 id="使用pymongo将item写入MongoDB"><a href="#使用pymongo将item写入MongoDB" class="headerlink" title="使用pymongo将item写入MongoDB"></a>使用pymongo将item写入MongoDB</h3><p>MongoDB的地址和数据库名称在Scrapy setttings.py配置文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">MONGO_URI = &quot;mongodb://localhost:27017&quot;;</div><div class="line">MONGO_DATABASE = &quot;soufang&quot;;</div></pre></td></tr></table></figure>
<p>在pipelines.py中定义一个函数连接数据库，然后定义一个函数去处理被解析的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">import pymongo</div><div class="line"></div><div class="line">class MongoPipeline(object):</div><div class="line"></div><div class="line">    collection_name = &apos;scrapy_items&apos;</div><div class="line"></div><div class="line">    def __init__(self, mongo_uri, mongo_db):</div><div class="line">        self.mongo_uri = mongo_uri</div><div class="line">        self.mongo_db = mongo_db</div><div class="line"></div><div class="line">    @classmethod</div><div class="line">    def from_crawler(cls, crawler):</div><div class="line">        return cls(</div><div class="line">            mongo_uri=crawler.settings.get(&apos;MONGO_URI&apos;),</div><div class="line">            mongo_db=crawler.settings.get(&apos;MONGO_DATABASE&apos;, &apos;items&apos;)</div><div class="line">        )</div><div class="line"></div><div class="line">    def open_spider(self, spider):</div><div class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</div><div class="line">        self.db = self.client[self.mongo_db]</div><div class="line"></div><div class="line">    def close_spider(self, spider):</div><div class="line">        self.client.close()</div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line">        self.db[self.collection_name].insert(dict(item))</div><div class="line">        return item</div></pre></td></tr></table></figure>
<h4 id="附录：Mac安装MongoDB"><a href="#附录：Mac安装MongoDB" class="headerlink" title="附录：Mac安装MongoDB"></a>附录：Mac安装MongoDB</h4><p>更新Homebrew的包数据库</p>
<p><code>brew update</code></p>
<p>安装MongoDB，安装完成后显示位置：/usr/local/Cellar/mongodb/3.4.9</p>
<p><code>brew install mongodb</code></p>
<blockquote>
<p>To have launchd start mongodb now and restart at login:<br>  brew services start mongodb<br>Or, if you don’t want/need a background service you can just run:<br>  mongod –config /usr/local/etc/mongod.conf</p>
</blockquote>
<p>打开.bash_profile</p>
<p><code>cd ~</code></p>
<p><code>vim .bash_profile</code></p>
<p>在.bash_profile文件中加入下面语句，i进行编辑，esc退出编辑，:wq保存并推出</p>
<p><code>export PATH=/usr/local/Cellar/mongodb/3.4.4/bin:$PATH</code></p>
<p>创建/data/db目录</p>
<p><code>sudo mkdir -p /data/db</code></p>
<p>运行mongodb，默认数据库目录即为 /data/db</p>
<p><code>mongod</code></p>
<p>另一个窗口运行，开始各种数据库指令</p>
<p><code>mongo</code></p>
<p>显示已经存在的数据库</p>
<p><code>show dbs</code>    </p>
<p>退出</p>
<p><code>exit</code></p>
</div></article></div></main><footer><div class="paginator"><a href="/2017/09/18/利用海盗模型建立数据分析维度/" class="prev">上一篇</a><a href="/2017/09/11/产品需要知道的认知与设计准则/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'mihawkmah';
var disqus_identifier = '2017/09/12/Scrapy爬虫入门-二-数据处理与存储/';
var disqus_title = 'Scrapy爬虫入门[二] | 数据处理与存储';
var disqus_url = 'http://Mihawkmah.github.io/2017/09/12/Scrapy爬虫入门-二-数据处理与存储/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//mihawkmah.disqus.com/count.js" async></script><div class="copyright"><p>© 2017 - 2018 <a href="http://Mihawkmah.github.io">Mihawkmah</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>