<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Scrapy爬虫入门[二]-数据处理与存储 · 末小山 | 解锁产品&数据成就</title><meta name="description" content="Scrapy爬虫入门[二]-数据处理与存储 - Mihawkmah"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://mihawkmah.github.io/atom.xml" title="末小山 | 解锁产品&amp;数据成就"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">我的</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">胶囊</a></li><li class="nav-list-item"><a href="https://www.instagram.com/mihawkmah/" target="_blank" class="nav-list-link">摄影</a></li><li class="nav-list-item"><a href="https://github.com/mihawkmah" target="_blank" class="nav-list-link">足迹</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Scrapy爬虫入门[二]-数据处理与存储</h1><div class="post-info">2017年9月12日</div><div class="post-content"><h3 id="使用项目item"><a href="#使用项目item" class="headerlink" title="使用项目item"></a>使用项目item</h3><p>Scrapy提供了item类，是装载抓取数据的容器。声明类似于Flask框架中的Models。在tutorial目录下的iterms.py文件编辑。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line">class Product(scrapy.Item):</div><div class="line">    name = scrapy.Field()</div><div class="line">    price = scrapy.Field()</div><div class="line">    stock = scrapy.Field()</div><div class="line">    last_updated = scrapy.Field(serializer=str)</div></pre></td></tr></table></figure>
<a id="more"></a>
<h4 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">product [ &apos;last_updated&apos; ]  =  &apos;today&apos;</div></pre></td></tr></table></figure>
<h4 id="查询数据"><a href="#查询数据" class="headerlink" title="查询数据"></a>查询数据</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"># 查询product的数据主键</div><div class="line">product.keys()</div><div class="line"># 查询product中的所有数据</div><div class="line">product.items()</div></pre></td></tr></table></figure>
<h4 id="复制项目"><a href="#复制项目" class="headerlink" title="复制项目"></a>复制项目</h4><p><code>product2 = product.copy()</code></p>
<h4 id="从项目创建字典"><a href="#从项目创建字典" class="headerlink" title="从项目创建字典"></a>从项目创建字典</h4><p><code>dict(product)</code></p>
<h3 id="使用项目管道item-Pipeline"><a href="#使用项目管道item-Pipeline" class="headerlink" title="使用项目管道item Pipeline"></a>使用项目管道item Pipeline</h3><p>项目被蜘蛛住区后，它被发送到项目管道</p>
<h4 id="典型用途"><a href="#典型用途" class="headerlink" title="典型用途"></a>典型用途</h4><ul>
<li>清理HTML数据验证</li>
<li>验证爬取的数据（检查项目是否包含特定字段）</li>
<li>查重（并删除）</li>
<li>将刮取的项目存储在数据库中</li>
</ul>
<h4 id="pipelines-py中编写item-pipeline"><a href="#pipelines-py中编写item-pipeline" class="headerlink" title="pipelines.py中编写item pipeline"></a>pipelines.py中编写item pipeline</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">class DoubanPipeline(object):</div><div class="line">    def process_item(self, item, spider):</div><div class="line">        return item</div></pre></td></tr></table></figure>
<h3 id="使用pymongo将item写入MongoDB"><a href="#使用pymongo将item写入MongoDB" class="headerlink" title="使用pymongo将item写入MongoDB"></a>使用pymongo将item写入MongoDB</h3><p>MongoDB的地址和数据库名称在Scrapy setttings.py配置文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ITEM_PIPELINES = [stack.pipelines.MongoDBPipeline,]</div><div class="line"> </div><div class="line">MONGODB_SERVER = localhost;</div><div class="line">MONGODB_PORT = 27017</div><div class="line">MONGODB_DB = stackoverflow;</div><div class="line">MONGODB_COLLECTION = questions;</div></pre></td></tr></table></figure>
<p>在pipelines.py中定义一个函数连接数据库，然后定义一个函数去处理被解析的数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">import pymongo</div><div class="line">from scrapy.conf import settings</div><div class="line"></div><div class="line">class MongoDBPipeline(object):</div><div class="line">    def __init__(self):</div><div class="line">        connection = pymongo.Connection(</div><div class="line">            settings[&apos;MONGODB_SERVER&apos;],</div><div class="line">            settings[&apos;MONGODB_PORT&apos;]</div><div class="line">        )</div><div class="line">        db = connection[settings[&apos;MONGODB_DB&apos;]]</div><div class="line">        self.collection = db[settings[&apos;MONGODB_COLLECTION&apos;]]</div><div class="line"></div><div class="line">def process_item(self, item, spider):</div><div class="line">        valid = True</div><div class="line">        for data in item:</div><div class="line">            if not data:</div><div class="line">                valid = False</div><div class="line">                raise DropItem(&quot;Missing &#123;0&#125;!&quot;.format(data))</div><div class="line">        if valid:</div><div class="line">            self.collection.insert(dict(item))</div><div class="line">            log.msg(&quot;Question added to MongoDB database!&quot;,</div><div class="line">                    level=log.DEBUG, spider=spider)</div><div class="line">        return item</div></pre></td></tr></table></figure>
</div></article></div></main><footer><div class="paginator"><a href="/2017/09/11/产品需要知道的认知与设计准则/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'mihawkmah';
var disqus_identifier = '2017/09/12/Scrapy爬虫入门-二-数据处理与存储/';
var disqus_title = 'Scrapy爬虫入门[二]-数据处理与存储';
var disqus_url = 'https://mihawkmah.github.io/2017/09/12/Scrapy爬虫入门-二-数据处理与存储/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//mihawkmah.disqus.com/count.js" async></script><div class="copyright"><p>© 2017 <a href="https://mihawkmah.github.io">Mihawkmah</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>