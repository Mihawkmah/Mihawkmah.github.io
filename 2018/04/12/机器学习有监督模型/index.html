<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> 机器学习有监督模型 · 末小山 | 锦鲤无妙棋</title><meta name="description" content="机器学习有监督模型 - Mihawkmah"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="http://Mihawkmah.github.io/atom.xml" title="末小山 | 锦鲤无妙棋"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">HOME</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="https://github.com/mihawkmah" target="_blank" class="nav-list-link">GITHUB</a></li><li class="nav-list-item"><a href="http://mihawkmah.lofter.com/" target="_blank" class="nav-list-link">LOFTER</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">机器学习有监督模型</h1><div class="post-info">2018年4月12日</div><div class="post-content"><h3 id="明确问题类别"><a href="#明确问题类别" class="headerlink" title="明确问题类别"></a>明确问题类别</h3><h4 id="有监督还是无监督"><a href="#有监督还是无监督" class="headerlink" title="有监督还是无监督"></a>有监督还是无监督</h4><p>根据输入来选择：</p>
<ul>
<li>有标记的数据就是监督学习问题</li>
<li>无标记数据，找结构就是无监督学习问题</li>
</ul>
<h4 id="有监督学习：回归、分类"><a href="#有监督学习：回归、分类" class="headerlink" title="有监督学习：回归、分类"></a>有监督学习：回归、分类</h4><p>根据输出判断：</p>
<ul>
<li>输出是数字，这是回归问题</li>
<li>如果输出是类别，那么是分类问题</li>
</ul>
<a id="more"></a>
<h3 id="如何选择算法"><a href="#如何选择算法" class="headerlink" title="如何选择算法"></a>如何选择算法</h3><ul>
<li>从简单算法开始，比如线性回归、SVM，现成模型快速看结果，如果结果不错，很可能各种算法都可以得到好结果</li>
<li>结果不好，那么可能问题比较困难，可能这是一个非线性问题，是否需要进行特征工程，使用Kernel函数（SVM），使用深度学习进行特征工程（数据够多）。又或者特征太多，数据少，可能需要考虑特征选择，比如LASSO或者PCA</li>
<li>实践，试错！</li>
</ul>
<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><h4 id="单变量线性回归、多变量线性回归、多项式回归、Ridge、LASSO"><a href="#单变量线性回归、多变量线性回归、多项式回归、Ridge、LASSO" class="headerlink" title="单变量线性回归、多变量线性回归、多项式回归、Ridge、LASSO"></a>单变量线性回归、多变量线性回归、多项式回归、Ridge、LASSO</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># 将数据分为训练集、测试集</div><div class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)</div><div class="line"></div><div class="line"># 使用训练数据对模型进行训练，得到lr</div><div class="line">lr = LinearRegression().fit(X_train,y_train)</div><div class="line"># 模型正则化 Ridge回归，alpha的值越大就越严格，值越小越灵活</div><div class="line"># ridge10 = Ridge(alpha=10).fit(X_train,y_train)</div><div class="line"># 模型正则化 Lasso回归，alpha的值越大使用的特征数量越少，值越小惩罚越轻特征越多模型越复杂</div><div class="line"># lasso01 = Lasso(alpha=0.1).fit(X_train,y_train)</div><div class="line"># 打印出Lasso使用了几个特征</div><div class="line"># print(format(np.sum(lasso01.coef_!=0)))</div><div class="line"></div><div class="line"># 通过模型lr函数做预测</div><div class="line">y_train_pred = lr.predict(X_train)</div><div class="line">y_test_pred = lr.predict(X_test)</div><div class="line"></div><div class="line"># 通过图形直观看模型质量</div><div class="line">plt.scatter(y_train_pred,y_train_pred-y_train,c=&apos;blue&apos;)</div><div class="line">plt.scatter(y_test_pred,y_test_pred-y_test,c=&apos;red&apos;)</div><div class="line"></div><div class="line"># 通过模型得分看模型质量</div><div class="line">print(&apos;训练数据得分：&#123;:.3f&#125;&apos;.format(lr.score(X_train,y_train)))</div><div class="line">print(&apos;测试数据得分：&#123;:.3f&#125;&apos;.format(lr.score(X_test,y_test)))</div></pre></td></tr></table></figure>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 1.1 引入数据</div><div class="line">cancer = load_breast_cancer()</div><div class="line"># 1.2 拆分测试集、训练集</div><div class="line">X_train,X_test,y_train,y_test=train_test_split(cancer.data,cancer.target,stratify=cancer.target,random_state=0)</div><div class="line"></div><div class="line"># — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</div><div class="line"></div><div class="line"># 线性支持向量机</div><div class="line">svc = LinearSVC(C=0.1,random_state=0).fit(X_train,y_train) # 默认C=1,越大模型越灵活</div><div class="line">print(&apos;线性训练数据得分：&#123;:.3f&#125;&apos;.format(svc.score(X_train,y_train)))</div><div class="line">print(&apos;线性测试数据得分：&#123;:.3f&#125;&apos;.format(svc.score(X_test,y_test)))</div><div class="line"></div><div class="line"># 引入核函数的支持向量机</div><div class="line">svm = SVC(kernel=&apos;rbf&apos;, random_state=0, gamma=2, C=10) #gamma越大越容易导致过拟合</div><div class="line">svm.fit(X_train,y_train)</div><div class="line">print(&apos;引入核函数训练数据得分：&#123;:.3f&#125;&apos;.format(svm.score(X_train,y_train)))</div><div class="line">print(&apos;引入核函数测试数据得分：&#123;:.3f&#125;&apos;.format(svm.score(X_test,y_test)))</div></pre></td></tr></table></figure>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><h4 id="ID3（不能处理连续分布的数据特征）、C4-5（支持连续分布的数据特征）、CART（分类回归树）"><a href="#ID3（不能处理连续分布的数据特征）、C4-5（支持连续分布的数据特征）、CART（分类回归树）" class="headerlink" title="ID3（不能处理连续分布的数据特征）、C4.5（支持连续分布的数据特征）、CART（分类回归树）"></a>ID3（不能处理连续分布的数据特征）、C4.5（支持连续分布的数据特征）、CART（分类回归树）</h4><p>1、特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着不同量化评估标准，从而衍生出不同的决策树算法<br>2、决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。树结构来说，递归结构是最容易理解的方式<br>3、剪枝：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种</p>
<h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><h4 id="朴素贝叶斯算法做了一个假设“认为各个特征相互独立”"><a href="#朴素贝叶斯算法做了一个假设“认为各个特征相互独立”" class="headerlink" title="朴素贝叶斯算法做了一个假设“认为各个特征相互独立”"></a>朴素贝叶斯算法做了一个假设“认为各个特征相互独立”</h4><ul>
<li>多项式模型（用于文本分类，特征是单词，值是单词的出现次数）</li>
<li>高斯模型（将连续型变量转化成离散型变量，比如身高170-178转化为特征值是1）</li>
<li>伯努利模型（特征取值是布尔型，即True或False。文本分类中，就是一个特征有没有在一个文档中出现）</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"># 导入康奈尔大学网站的2M影评数据集，tokens下的文件夹neg、pos被识别是target</div><div class="line">movie_reviews = load_files(&apos;./tokens&apos;)</div><div class="line">movie_data = movie_reviews.data</div><div class="line">movie_target = movie_reviews.target</div><div class="line"></div><div class="line"># 有多少条样本数据</div><div class="line">len(movie_data)</div><div class="line"></div><div class="line"># 将文本型的数据转化为TF-IDF矩阵</div><div class="line">count_vec = TfidfVectorizer(binary = False, decode_error=&apos;ignore&apos;, stop_words=&apos;english&apos;)</div><div class="line"># 分为训练集、测试集</div><div class="line">x_train,x_test,y_train,y_test = train_test_split(movie_data, movie_target, test_size=0.2)</div><div class="line"># 将训练数据转化为词频</div><div class="line">x_train = count_vec.fit_transform(x_train)</div><div class="line"># 利用上一步得到的count_vec，将测试数据也转化为词频</div><div class="line">x_test= count_vec.transform(x_test)</div><div class="line"></div><div class="line"># MultinomialNB，这个分类器以出现次数作为特征值，我们使用的TF-IDF也能符合这类分布。GaussianNB适用于高斯分布（正态分布）的特征，而BernoulliNB适用于伯努利分布（二值分布）的特征</div><div class="line">clf = MultinomialNB().fit(x_train,y_train)</div><div class="line">doc_class_predicted = clf.predict(x_test)</div><div class="line"></div><div class="line"># 查看有多少比率预测正确了</div><div class="line">print(np.mean(doc_class_predicted == y_test))</div></pre></td></tr></table></figure>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># 引入数据</div><div class="line">X,y = make_moons(n_samples=100, noise=0.25, random_state=3)</div><div class="line"># 分为训练集、测试集</div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)</div><div class="line"># 训练得到模型，solver指定算法为lbfgs，hidden_layer_sizes指定2层，每层10个神经元。使用tanh激活函数</div><div class="line">mlp = MLPClassifier(solver=&apos;lbfgs&apos;, activation=&apos;tanh&apos;, random_state=0, hidden_layer_sizes=[10, 10]).fit(X_train, y_train)</div></pre></td></tr></table></figure>
</div></article></div></main><footer><div class="paginator"><a href="/2018/04/10/机器学习入门基础/" class="next">下一篇</a></div><div id="disqus_thread"></div><script>var disqus_shortname = 'mihawkmah';
var disqus_identifier = '2018/04/12/机器学习有监督模型/';
var disqus_title = '机器学习有监督模型';
var disqus_url = 'http://Mihawkmah.github.io/2018/04/12/机器学习有监督模型/';
(function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();</script><script id="dsq-count-scr" src="//mihawkmah.disqus.com/count.js" async></script><div class="copyright"><p>© 2017 - 2018 <a href="http://Mihawkmah.github.io">Mihawkmah</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>